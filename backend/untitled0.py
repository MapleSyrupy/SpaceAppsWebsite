# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I8Q2lVoHAbS0xaxfoKdx1JkigFqWU0hu
"""

from google.colab import files

uploaded = files.upload()

import os
for file_name in uploaded.keys():
    print("Uploaded:", file_name)

import pandas as pd

csv_path = "cumulative_2025.10.04_11.32.32.csv"

df = pd.read_csv(csv_path, sep=",", comment="#", low_memory=False)
print(df.shape)
print(df.head())

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import RobustScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.metrics import (
    accuracy_score, classification_report,
    confusion_matrix, precision_recall_curve,
    average_precision_score
)

disp_col = "koi_disposition"

df = df[df[disp_col].isin(["CONFIRMED", "FALSE POSITIVE"])].copy()
df["LABEL"] = (df[disp_col] == "CONFIRMED").astype(int)
print("Label counts:\n", df["LABEL"].value_counts())

drop_cols = {"LABEL", disp_col, "kepoi_name", "kepler_name"}
num_cols = [c for c in df.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(df[c])]

num_cols = [c for c in num_cols if not df[c].isnull().all()]

X = df[num_cols]
y = df["LABEL"]
print("Number of features:", len(num_cols))

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, stratify=y, random_state=42
)

pre = ColumnTransformer([
    ("num", Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", RobustScaler())
    ]), num_cols)
])

models = {
    "KNN (k=4)": KNeighborsClassifier(n_neighbors=4),
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight="balanced"),
    "Decision Tree": tree.DecisionTreeClassifier(max_depth=8, class_weight="balanced", random_state=0)
}

for name, clf in models.items():
    pipe = Pipeline([("pre", pre), ("clf", clf)])
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)


    acc = accuracy_score(y_test, y_pred)
    if hasattr(pipe, "predict_proba"):
        y_score = pipe.predict_proba(X_test)[:, 1]
    else:
        y_score = y_pred
    ap = average_precision_score(y_test, y_score)

    print(f"\n=== {name} ===")
    print("Accuracy:", f"{acc:.4f}")
    print("PR-AUC:", f"{ap:.4f}")
    print(classification_report(y_test, y_pred, digits=4))

    cm = confusion_matrix(y_test, y_pred, labels=[0, 1])
    fig, ax = plt.subplots(1, 2, figsize=(10,4))

    im = ax[0].imshow(cm, cmap="Blues")
    ax[0].set_title(f"{name} - Confusion Matrix")
    ax[0].set_xlabel("Predicted")
    ax[0].set_ylabel("True")
    ax[0].set_xticks([0,1]); ax[0].set_yticks([0,1])
    ax[0].set_xticklabels(["Non-planet","Planet"])
    ax[0].set_yticklabels(["Non-planet","Planet"])
    for (i,j), v in np.ndenumerate(cm):
        ax[0].text(j, i, str(v), ha="center", va="center")
    fig.colorbar(im, ax=ax[0])

    pr, rc, _ = precision_recall_curve(y_test, y_score)
    ax[1].plot(rc, pr)
    ax[1].set_title(f"{name} - Precision-Recall")
    ax[1].set_xlabel("Recall")
    ax[1].set_ylabel("Precision")
    ax[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()